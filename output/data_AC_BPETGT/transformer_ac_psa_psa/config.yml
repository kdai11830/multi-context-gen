GPT_representation_loc: src
GPT_representation_mode: none
accum_count: '18'
adagrad_accumulator_init: 0
adam_beta1: '0.9'
adam_beta2: '0.998'
attn_dropout: -1
attn_hidden: -1
audio_enc_pooling: '1'
average_decay: 0
average_every: 1
base_encoder_type: rnn
batch_size: '2200'
batch_type: tokens
cnn_kernel_width: 3
config: encoder-agnostic-model/config/ac/transformer_ac_psa.yml
data: data/AC_BPETGT
dec_heads: 8
dec_layers: '12'
dec_lr_factor: 1.0
dec_rnn_size: 500
decay_method: stlr
decay_steps: 10000
decoder_type: transformer
disc_ft: '1.3'
dropout: '0.1'
enc_heads: 8
enc_layers: '4'
enc_rnn_size: 500
encoder_type: transformer
epochs: 0
exp: ''
exp_host: ''
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
generator_function: softmax
global_attention: general
global_attention_function: softmax
gpt2_init_embanddec: 'true'
gpt2_params_path: encoder-agnostic-model/gpt2/models/124M/
gpt2_params_std: -1
gpu_backend: nccl
gpu_ranks:
- 0
gpu_verbose_level: 0
gpuid: []
heads: '12'
image_channel_size: 3
input_feed: 1
keep_checkpoint: '3'
label_smoothing: '0.1'
lambda_coverage: 1
layers: -1
learning_rate: 1e-3
learning_rate_decay: 0.5
load_uncond_from: ''
log_file: ''
log_file_level: '0'
loss_scale: 0
master_ip: localhost
master_port: 10000
max_generator_batches: '2'
max_grad_norm: '0.0'
max_relative_positions: 0
model_dtype: fp32
model_type: text
normalization: tokens
optim: adam
param_init: '0.0'
param_init_glorot: 'true'
position_encoding: 'true'
position_encoding_ctxsize: '1024'
position_encoding_learned_dec: 'true'
report_every: '40'
reset_optim: none
rnn_size: '768'
rnn_type: LSTM
run_name: psa
sample_rate: 16000
save_checkpoint_steps: '500'
save_model: output/data_AC_BPETGT/transformer_ac_psa_psa/checkpoints/model
seed: '123'
self_attn_type: scaled-dot
share_decoder_embeddings: 'true'
src_word_vec_size: 500
start_decay_steps: 50000
stlr_ratio: 32
tensorboard: 'true'
tensorboard_log_dir: output/data_AC_BPETGT/tblogs/transformer_ac_psa_psa
tgt_word_vec_size: 500
train_from: ''
train_steps: '2500'
transformer_ff: '3072'
truncated_decoder: 0
use_GPT_version_psa: 'true'
valid_batch_size: 32
valid_steps: '40'
warmup_init_factor: 5000
warmup_steps: '250'
window_size: 0.02
word_vec_size: '768'
world_size: '1'
